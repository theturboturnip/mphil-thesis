\section{Fast-path calculations\label{chap:hardware:sec:fastpath}}
A fast-path check can be performed over various sets of elements.
The emulator chooses to perform a single fast-path check for each vector access, calculating the tight bounds before starting the actual access, but in hardware this may introduce prohibitive latency.
This section describes the general principles surrounding fast-paths for CHERI-RVV, notes the areas where whole-access fast-paths are difficult to calculate, and describes possible approaches for hardware.

\subsection{Possible fast-path outcomes}
In some cases, a failed address range check may not mean the access fails.
The obvious case is fault-only-first loads, where capability exceptions may be handled without triggering a trap.
Implementations may also choose to calculate wider bounds than accessed for the sake of simplicity, or even forego a fast-path check altogether.
Thus, a fast-path check can have four outcomes depending on the circumstances.

\begin{figure}[t]
\begin{subtable}{0.5\textwidth}
    \begin{tabular}{ll}
        \toprule
        Success & All accesses will succeed \\
        \midrule
        Failure & At least one access \emph{will}\\& raise an exception \\
        \midrule
        Likely-Failure & At least one access \emph{may} \\
        \emph{or} Unchecked & raise an exception \\
        \bottomrule
    \end{tabular}
    \caption{Possible fast-path outcomes}
\end{subtable}%
\hfill%
\begin{subfigure}{0.45\textwidth}
    \begin{algorithmic}
        \If{can calculate range}
            \If{range is within capability}
                \State Success
            \ElsIf{range is wide}
                \State Likely-Failure
            \ElsIf{fault-only-first}
                \State Likely-Failure
            \Else{}
                \State Failure
            \EndIf{}
        \Else{}
            \State Unchecked
        \EndIf{}
    \end{algorithmic}
    \caption{Algorithm}
\end{subfigure}
\caption{Fast-path outcomes}
\end{figure}

A Success means no per-access capability checks are required.
Likely-Failure and Unchecked results mean each access must be checked, to see if any of them actually raise an exception.
Unfortunately, accesses still need to be checked under Failure, because both precise and imprecise traps need to report the offending element in \code{vstart}\footnote{In very particular cases, e.g. unmasked unit-strided accesses where \code{nf = 1}, the capability bounds could be used to calculate what the offending element must have been. We believe this is too niche of a use case to investigate further, particularly given the complexity of the resulting hardware.}.

Because all archetypes may have Failure or Likely-Failure outcomes, hardware must provide a fallback slow-path for each archetype which checks/performs each access in turn.
In theory, a CHERI-RVV specification could relax the \code{vstart} requirement for imprecise traps, and state that all capability exceptions trigger imprecise traps.
In this case, only archetypes that produce Likely-Failure outcomes need the slow-path.
However, it is likely that for complexity reasons all masked accesses will use wide ranges, thus producing Likely-Failure outcomes and requiring slow-paths for all archetypes anyway.
Because the Likely-Failure and Failure cases require the slow-path anyway, computing the fast-path can only be worthwhile if Success is the common case.

\subsection{Whole-access fast-paths}\label{chap:hardware:subsec:wholeaccessfastpath}
It is technically possible to calculate a fast-path for the entirety of an access (see \cref{appx:fastpathfull}), but for some situations it may be equally/more expensive than checking each access.
For example, the bounds for masked accesses depend on finding the minimum and maximum active indices, which in hardware may require a linear scan.
Indexed accesses require finding the minimum/maximum offset values, which likely requires an expensive parallel reduction over all/some elements.
In these cases hardware implementations could defer to the slow-path on all masked/indexed accesses, or for masked accesses use the wider, unmasked bounds and generate Likely-Failure outcomes.
Unit and strided accesses are much easier to handle.

Arbitrarily strided accesses (which may have positive, negative, or zero-valued strides) are relatively simple to calculate.
After calculating the segment width (i.e. $\text{number of fields} * \text{element width}$) the full bounds just depends on the sign of the stride (\cref{eq:tightboundsstrided}).
Unit-stride accesses simplify this further, because the stride is equal to the segment width and guaranteed to be positive (\cref{eq:tightboundsunit}).

\newcommand{\vstart}{\code{vstart}}
% \newcommand{\vstartactive}{\code{vstart}_{\mathit{active}}}
\newcommand{\vstartactive}{\code{vstart}}
\newcommand{\evl}{\code{evl}}
% \newcommand{\evlactive}{\code{evl}_{\mathit{active}}}
\newcommand{\evlactive}{\code{evl}}
\newcommand{\baseaddr}{\code{base}}

\begin{mycapequ}[!ht]

\begin{equation}\label{eq:tightboundsstrided}
\baseaddr{}\ +\ \begin{cases}
        \code{[\vstartactive{} * \code{stride}, (\evlactive{} - 1) * \code{stride} + \code{nf} * \code{eew})} & \code{stride} \ge 0 \\
            
        \code{[(\evlactive{} - 1) * \code{stride}, \vstartactive{} * \code{stride} + \code{nf} * \code{eew})} & \code{stride} < 0
    \end{cases}
\end{equation}
\caption{Tight bounds for strided access}
\end{mycapequ}

\begin{mycapequ}[!ht]
\begin{equation}\label{eq:tightboundsunit}
    \baseaddr{}\ +\ \code{[\vstartactive{} * \code{nf} * \code{eew}, \evlactive{} * \code{nf} * \code{eew})}
\end{equation}
\caption{Tight bounds for unit-stride access}
\end{mycapequ}

Ultimately, the potential up-front latency seemed like a dealbreaker for this approach.
We turned our attention to fast-pathing smaller groups of elements.

\subsection{$m$-element known-range fast-paths}
A hardware implementation of a vector unit may be able to issue $m$ requests within a set range in parallel.
For example, elements in the same cache line may be accessible all at once.
In these cases, checking elements individually would either require $m$ parallel bounds checks, $m$ checks' worth of latency, or something inbetween.
In this subsection we consider a fast-path check for $m$ elements. 

% \todomark{pre-check: permissions}
Capability checks can be split into two steps: address-agnostic (e.g. permissions checks, bounds decoding) and address-dependent (e.g. bounds checks).
Address-agnostic steps can be performed before any bounds checking, and should add minimal start-up latency (bounds decoding must complete before the checks anyway, and permission checks can be performed in parallel).
Once the bounds are decoded the actual checks consist of minimal logic\footnote{Likely requires two arithmetic operations per element, for checking against the top and bottom bounds.}, so a fast-path must have very minimal logic to compete.

We first consider unit and strided accesses, and note two approaches.
First, one could amortize the checking logic cost over multiple sets of $m$ elements by operating in terms of cache lines.
Iterating through all accessed cache lines, and then iterating over the elements inside, allows the fast-path to hardcode the bounds width and do one check for multiple cycles of work (if cache lines contain more than $m$ elements).
Cache-line-aligned allocations benefit here, as all fast-path checks will be in-bounds i.e. Successful, but misaligned data is guaranteed to create at least one Likely-Failure outcome per access (requiring a slow-path check).
Calculating tight bounds for the $m$ accessed elements per cycle could address this.

% \todomark{unit/strided: by $m$ elements?}
For unit and strided accesses, the bounds occupied by $m$ elements is straightforward to calculate, as the addresses can be generated in order.
The minimum and maximum can then be picked easily to generate tight bounds.
An $m$-way multiplexer is still required for taking the minimum and maximum, because \code{evl} and \code{vstart} may not be $m$-aligned.
If $m$ is small, this also neatly extends to handle masked/inactive elements.
This may use less logic overall than $m$ parallel bounds checks, depending on the hardware platform\footnote{e.g. on FPGAs multiplexers can be relatively cheap.}, but it definitely uses more logic than the cache-line approach.
Clearly, there's a trade-off to be made.

Indexed fast-paths are more complicated, because the addresses are unsorted.
The two approaches above have different advantages for indexed accesses.
If the offsets/indices are spatially close, just not sorted, cache line checks may efficiently cover all elements.
An implementation could potentially cache the results, and refer back for each access, instead of trying to iterate through cache lines in order.
Otherwise a $m$-way parallel reduction could be performed to find the min and max, but that would likely take up more logic than $m$ comparisons.
This may be a moot point depending on the cache implementation though - if the $m$ accesses per cycle must be in the same cache line, and the addresses are spread out, you're limited to one access and therefore one check per cycle regardless.

In summary, there are fast-path checks that consume less logic than $m$ parallel checks in certain circumstances.
Even though a slow-path is always necessary, it can be implemented in a slow way (e.g. doing one check per cycle) to save on logic.
Particularly if other parts of the system rely on constraining the addresses accessed in each cycle, a fast-path check can take advantage of those constraints. 